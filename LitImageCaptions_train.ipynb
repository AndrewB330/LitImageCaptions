{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LitImageCaptions_train.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58tgt7_iOm8H"
      },
      "source": [
        "### **Load data from github repo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe1PsuTkOwAd"
      },
      "source": [
        "### **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajc4C3Pt8itj"
      },
      "source": [
        "from numpy import array\n",
        "from collections import defaultdict\n",
        "from pickle import load, dump\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical, plot_model\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import *\n",
        "from keras.layers.merge import add, Concatenate\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKGeqFIIPQaS"
      },
      "source": [
        "### **Specify dataset and load additional data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8XOSHCv-ort"
      },
      "source": [
        "dataset = 'coco' # 'coco' or 'flickr8k'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBoprroswvgn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14772cd3-f09a-44dd-e4ad-a155345f05ae"
      },
      "source": [
        "if dataset == 'coco':\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()\n",
        "  !gsutil cp gs://tpu330/* ./\n",
        "else:\n",
        "  !rm -rf LitImageCaptions\n",
        "  !git clone -q https://github.com/AndrewB330/LitImageCaptions\n",
        "  !unzip -o -q LitImageCaptions/features.zip\n",
        "  !cp LitImageCaptions/descriptions.txt descriptions.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://tpu330/coco_descriptions.txt...\n",
            "Copying gs://tpu330/coco_features.dat...\n",
            "Copying gs://tpu330/coco_features_ids.txt...\n",
            "Omitting prefix \"gs://tpu330/downloads/\". (Did you mean to do cp -r?)\n",
            "\n",
            "Operation completed over 3 objects/1.3 GiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpQdsTsvALj4"
      },
      "source": [
        "### **Functions for reading photo features and lables**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U36W4BqN6jDu"
      },
      "source": [
        "def load_clean_descriptions(dataset, identifiers, tokenizer=None):\n",
        "  c = open(f'{dataset}_descriptions.txt', 'r').read()\n",
        "  descriptions = defaultdict(lambda: [])\n",
        "  for line in c.split('\\n'):\n",
        "    tokens = line.split()\n",
        "    if len(tokens) < 2 or len(tokens) > 16:\n",
        "      continue\n",
        "    id, desc = tokens[0], tokens[1:]\n",
        "    if tokenizer is not None:\n",
        "      desc = tokenizer.texts_to_sequences([' '.join(desc)])\n",
        "      desc = tokenizer.sequences_to_texts(desc)[0].split()\n",
        "      any = False\n",
        "      for t in desc:\n",
        "        if tokenizer.word_counts[t] <= 10:\n",
        "          any = True\n",
        "          break\n",
        "      if any:\n",
        "        continue\n",
        "    desc = 'startseq ' + ' '.join(desc) + ' endseq'\n",
        "    descriptions[id].append(desc)\n",
        "  return [descriptions[id] for id in identifiers]\n",
        "\n",
        "def create_tokenizer(descriptions):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts([d for dd in descriptions for d in dd])\n",
        "  return tokenizer\n",
        "\n",
        "def max_length_compute(descriptions):\n",
        "  return max([len(d.split()) for dd in descriptions for d in dd])\n",
        "\n",
        "def read_dataset(dataset, test_size=2000):\n",
        "  c = open(f'{dataset}_identifiers.txt', 'r').read()\n",
        "  identifiers = [l.split('.')[0] for l in c.split('\\n')]\n",
        "  f = np.load(open(f'{dataset}_features.dat', 'rb'))\n",
        "  d = load_clean_descriptions(dataset, identifiers)\n",
        "  d = load_clean_descriptions(dataset, identifiers, create_tokenizer(d))\n",
        "  d = load_clean_descriptions(dataset, identifiers, create_tokenizer(d))\n",
        "  assert(len(f) == len(d))\n",
        "  return train_test_split(f, d, test_size=test_size, random_state=0)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfOvT6bQAUMQ"
      },
      "source": [
        "### **Data generator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRiHJ22L8oC8"
      },
      "source": [
        "def create_sequences_gen(tokenizer, max_length, descriptions, \n",
        "                         features, vocab_size, *args, \n",
        "                         batch_size=2048, infinite=False):\n",
        "  X1, X2, y = [], [], []\n",
        "  while True:\n",
        "    for feature, d in zip(features.copy(), descriptions):\n",
        "      for description in d:\n",
        "        seq = tokenizer.texts_to_sequences([description])[0]\n",
        "        for i in range(1, len(seq)):\n",
        "          in_seq, out_seq = seq[:i], seq[i]\n",
        "          in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "          out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "          X1.append(feature.reshape(1, -1))\n",
        "          X2.append(in_seq.reshape(1, -1))\n",
        "          y.append(out_seq.reshape(1, -1))\n",
        "          if len(X1) >= batch_size:\n",
        "            yield [np.vstack(X1), np.vstack(X2)], np.vstack(y)\n",
        "            X1, X2, y = [], [], []\n",
        "    if not infinite:\n",
        "      break\n",
        "  if len(X1) > 0:\n",
        "    yield [np.vstack(X1), np.vstack(X2)], np.vstack(y)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwW9JEP7AfIw"
      },
      "source": [
        "### **Read train and test data, initialize tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rNMLxYC-kCm",
        "outputId": "612dfca0-2508-49b5-e978-91c7e1dedd41"
      },
      "source": [
        "train_features, test_features, \\\n",
        "  train_descriptions, test_descriptions = read_dataset(dataset)\n",
        "\n",
        "print(f'Train dataset: {len(train_features)}')\n",
        "print(f'Test dataset: {len(test_features)}')\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "max_length = max_length_compute(train_descriptions)\n",
        "print('Description Length: %d' % max_length)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train dataset: 79920\n",
            "Test dataset: 2000\n",
            "Vocabulary Size: 5364\n",
            "Description Length: 19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INHCBEheAm6E"
      },
      "source": [
        "# **Model definition**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLln8It79yYT"
      },
      "source": [
        "def define_model(vocab_size, max_length):\n",
        "\tinputs1 = Input(shape=(4096,))\n",
        "\tfe1 = Dense(512, activation='relu')(inputs1)\n",
        "\tfe1 = Dropout(0.2)(fe1)\n",
        "\t# sequence model\n",
        "\tinputs2 = Input(shape=(max_length,))\n",
        "\tse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "\tse2 = Dropout(0.5)(se1)\n",
        "\tse3 = LSTM(256)(se2)\n",
        "\t# decoder model\n",
        "\tdecoder1 = Concatenate()([fe1, se3])\n",
        "\tdecoder2 = Dense(512, activation='relu')(decoder1)\n",
        "\tdecoder2 = Dropout(0.2)(decoder2)\n",
        "\tdecoder2 = Dense(512, activation='relu')(decoder2)\n",
        "\toutputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\t# tie it together [image, seq] [word]\n",
        "\tmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\tplot_model(model, to_file='model.png', show_shapes=True)\n",
        "\treturn model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3gEWqmU93-r"
      },
      "source": [
        "model = define_model(vocab_size, max_length)\n",
        "load_img('model.png')\n",
        "\n",
        "version = 'v5'\n",
        "filepath = version + '_model-val_loss{val_loss:.3f}.h5'\n",
        "dump(tokenizer, open(version + '_tokenizer.pickle', 'wb'))\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', \n",
        "                             verbose=1, save_best_only=True, mode='min')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeW9QOFvAr22"
      },
      "source": [
        "# **Model training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8cOAPoY7mVz",
        "outputId": "bca06259-fe69-4e69-a282-251cf46e6074"
      },
      "source": [
        "gen = create_sequences_gen(tokenizer, max_length, train_descriptions, train_features, vocab_size, infinite=True)\n",
        "for j in range(20):\n",
        "  gen_validation = create_sequences_gen(tokenizer, max_length, test_descriptions, test_features, vocab_size)\n",
        "  model.fit(gen, epochs=1, verbose=1, steps_per_epoch=512, callbacks=[checkpoint], validation_data=gen_validation)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 82/512 [===>..........................] - ETA: 1:04 - loss: 5.8632"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}